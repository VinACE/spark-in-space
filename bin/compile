#!/usr/bin/env bash
# bin/compile <build-dir> <cache-dir> <env-dir>
# heroku inline buildpack

set -e

build_dir=$1
cache_dir=$2
env_dir=$3
bp_dir=$(dirname $(dirname $0))

cd $build_dir

spark_version=1.6.1
hadoop_version=2.6
hadoop_aws_shade=hadoop-aws-shaded-no-jackson-0.1-SNAPSHOT-shaded.jar

if [ -e "$build_dir/.spark.version"]; then
    spark_version=$(cat "$build_dir/.spark.version" | xargs)
    echo "Detected spark version: ${spark_version} configured in .spark.version"
    if [ "$spark_version" -eq "2.0.0" ]; then
      hadoop_version=2.7
      hadoop_aws_shade=hadoop-aws-shaded-no-jackson-0.2-hadoop-2.7-SNAPSHOT-shaded.jar
    elif [ "$spark_version" -eq "1.6.1" ]; then
      hadoop_version=2.6
    else
      echo "Only 1.6.1 or 2.0.0 are currently supported as .spark.version, exiting."
      exit 1
    end
end

fetch_spark_tarball() {
    local tarball_file="spark-${spark_version}-bin-hadoop${hadoop_version}.tgz"
    local stack="cedar-14"
    local spark_tarball_url="https://s3-external-1.amazonaws.com/heroku-spark/spark-${spark_version}-bin-hadoop${hadoop_version}.tgz"
    local dest_path="$cache_dir/$stack/$tarball_file"

    if [ -f "$dest_path" ]; then
        echo -n "cat $dest_path"
    else
        echo -n "curl -s -L $spark_tarball_url > $dest_path && cat $dest_path"
    fi
}

start() {
  echo -n "-----> $@..."
}

finish() {
  echo " Done."
}


start Installing Spark
$(fetch_spark_tarball) | tar xzC $build_dir
mv $build_dir/spark-$spark_version-bin-hadoop$hadoop_version $build_dir/spark-home
finish Installing Spark


start Configuring Spark
rm -rf $build_dir/spark-home/lib/spark-examples-*
rm -rf $build_dir/spark-home/yarn
mv $bp_dir/conf/spark-env.sh $build_dir/spark-home/conf
finish Configuring Spark

start Installing s3a:// HDFS Support
curl -s -L https://s3.amazonaws.com/heroku-spark/libhadoop.so.1.0.0 > $build_dir/spark-home/lib/libhadoop.so
curl -s -L https://s3.amazonaws.com/heroku-spark/libhdfs.so.0.0.0 > $build_dir/spark-home/lib/libhdfs.so
curl -s -L https://s3.amazonaws.com/heroku-spark/$hadoop_aws_shade > $build_dir/spark-home/lib/hadoop-aws-shaded.jar
finish Installing s3a:// HDFS Support



