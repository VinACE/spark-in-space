#!/usr/bin/env bash
# bin/compile <build-dir> <cache-dir> <env-dir>
# heroku inline buildpack

set -e

build_dir=$1
cache_dir=$2
env_dir=$3
bp_dir=$(dirname $(dirname $0))

notice() {
  echo "-----> $@..."
}

start() {
  echo -n "-----> $@..."
}

finish() {
  echo " Done."
}

cd $build_dir

spark_version=1.6.1
hadoop_version=2.6
hadoop_aws_shade_27=hadoop-aws-shaded-no-jackson-0.2-hadoop-2.7-SNAPSHOT-shaded.jar
hadoop_aws_shade_26=hadoop-aws-shaded-no-jackson-0.1-SNAPSHOT-shaded.jar
hadoop_aws_shade=$hadoop_aws_shade_26

if [ -f "$build_dir/.spark.version" ]; then
    spark_version=$(cat "$build_dir/.spark.version" | xargs)
    notice "Detected spark version: ${spark_version} configured in .spark.version"
    if [ "$spark_version" == "2.0.0" ]; then
      hadoop_version=2.7
      hadoop_aws_shade=$hadoop_aws_shade_27
    elif [ "$spark_version" == "1.6.1" ]; then
      hadoop_version=2.6
    elif [ "$spark_version" == "1.4.1" ]; then
      hadoop_version=2.6
    else
      notice "Only 1.4.1, 1.6.1, or 2.0.0 are currently supported as .spark.version, exiting."
      exit 1
    fi
else
  notice "Defaulting to Spark 1.6.1 for Hadoop 2.6"
fi

fetch_spark_tarball() {
    local tarball_file="spark-${spark_version}-bin-hadoop${hadoop_version}.tgz"
    local stack="cedar-14"
    local spark_tarball_url="https://s3-external-1.amazonaws.com/heroku-spark/spark-${spark_version}-bin-hadoop${hadoop_version}.tgz"
    local dest_path="$cache_dir/$stack/$tarball_file"

    if [ -f "$dest_path" ]; then
        echo -n "cat $dest_path"
    else
        echo -n "curl -s -L $spark_tarball_url > $dest_path && cat $dest_path"
    fi
}

cautious_copy() {
  local source_file=$1
  local dest_file=$2
  if [ -f "${dest_file}" ]
  then
    echo "-----> Skipping copy, destination exists: ${dest_file}"
  else
    echo "-----> Copying into build: ${source_file}"
    cp $source_file $dest_file
  fi
}

start Installing Spark
$(fetch_spark_tarball) | tar xzC $build_dir
mv $build_dir/spark-$spark_version-bin-hadoop$hadoop_version $build_dir/spark-home
cautious_copy $bp_dir/bin/master-url   $build_dir/bin/master-url
cautious_copy $bp_dir/bin/spark-master $build_dir/bin/spark-master
cautious_copy $bp_dir/bin/spark-shell  $build_dir/bin/spark-shell
cautious_copy $bp_dir/bin/spark-submit $build_dir/bin/spark-submit
cautious_copy $bp_dir/bin/spark-worker $build_dir/bin/spark-worker
finish Installing Spark


start Configuring Spark
rm -rf $build_dir/spark-home/lib/spark-examples-*
rm -rf $build_dir/spark-home/yarn
cautious_copy $bp_dir/conf/spark-env.sh            $build_dir/conf/spark-env.sh
cautious_copy $bp_dir/conf/log4j.properties.erb    $build_dir/conf/log4j.properties.erb
cautious_copy $bp_dir/conf/spark-defaults.conf.erb $build_dir/conf/spark-defaults.conf.erb
finish Configuring Spark

start Installing s3n:// and s3a:// HDFS Support
mkdir -p $build_dir/spark-home/lib
curl -s -L https://s3.amazonaws.com/heroku-spark/libhadoop.so.1.0.0 > $build_dir/spark-home/lib/libhadoop.so
curl -s -L https://s3.amazonaws.com/heroku-spark/libhdfs.so.0.0.0 > $build_dir/spark-home/lib/libhdfs.so
curl -s -L https://s3.amazonaws.com/heroku-spark/$hadoop_aws_shade > $build_dir/spark-home/lib/hadoop-aws-shaded.jar
curl -s -L https://s3.amazonaws.com/heroku-spark/twitter-hadoop-lzo/liblzo2.so.2.0.0 > $build_dir/spark-home/lib/liblzo2.so
curl -s -L https://s3.amazonaws.com/heroku-spark/twitter-hadoop-lzo/libgplcompression.so.0.0.0 > $build_dir/spark-home/lib/libgplcompression.so
curl -s -L https://s3.amazonaws.com/heroku-spark/twitter-hadoop-lzo/hadoop-lzo-0.4.20-SNAPSHOT.jar > $build_dir/spark-home/lib/hadoop-lzo.jar
finish Installing s3a:// HDFS Support



